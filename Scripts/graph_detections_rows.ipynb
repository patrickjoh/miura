{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomalies detected by all versions of RePad2 and MIURA\n",
    "# Plots true anomalies in red on top of the main dataset plot\n",
    "# Top plot is the main dataset, and the following subplots are the detected anomalies\n",
    "# Detections are show as binary series, where 1 indicates an anomaly\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from influxdb_client import InfluxDBClient\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# InfluxDB settings\n",
    "influxdb_url = \"http://localhost:8086\"\n",
    "token = \"random_token\"\n",
    "org = \"ORG\"\n",
    "bucket = \"system_state\"\n",
    "dataset = \"TEMP_org\" # Name of the measurements in the bucket\n",
    "dataset_name = \"TEMP\" # Name of the dataset for the plot\n",
    "bucket_ano = \"anomalies\" # Name of the bucket containing the anomaly markers\n",
    "dataset_labels = \"labels_TEMP_seq\" # Name of the measurements in the bucket containing the labels\n",
    "dataset_results = \"repad2-temp-result-minmax\" # Could be any of the detection results (Made by EVAL-versions)\n",
    "start_time = \"1997-04-10T00:00:00Z\"\n",
    "\n",
    "measurements = [\n",
    "#    f\"{dataset}\"\n",
    "    f\"miura-{dataset_name.lower()}-detection-minmax\",\n",
    "    f\"miura-{dataset_name.lower()}-detection-robust\",\n",
    "    f\"miura-{dataset_name.lower()}-detection-standard\",\n",
    "    f\"repad2-{dataset_name.lower()}-detection-minmax\",\n",
    "    f\"repad2-{dataset_name.lower()}-detection-robust\",\n",
    "    f\"repad2-{dataset_name.lower()}-detection-standard\",\n",
    "]\n",
    "\n",
    "names = [\n",
    "    \"MIURA-MinMax\",\n",
    "    \"MIURA-Robust\",\n",
    "    \"MIURA-Standard\",\n",
    "    \"RePAD2-MinMax\",\n",
    "    \"RePAD2-Robust\",\n",
    "    \"RePAD2-Standard\",\n",
    "]\n",
    "\n",
    "# Instantiate the InfluxDB client\n",
    "client = InfluxDBClient(url=influxdb_url, token=token, org=org)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(len(measurements) + 1, 1, figsize=(12, 5), sharex=True, gridspec_kw = {'wspace':0.15, 'hspace':0.15}, dpi=300)\n",
    "\n",
    "# Main dataset plot\n",
    "dataset_query = f'''\n",
    "from(bucket: \"{bucket}\")\n",
    "    |> range(start: time(v: \"{start_time}\"))\n",
    "    |> filter(fn: (r) => r[\"_measurement\"] == \"{dataset}\")\n",
    "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "'''\n",
    "dataset_df = query_api.query_data_frame(query=dataset_query)\n",
    "for i, ax in enumerate(axs):\n",
    "    if i == 0:\n",
    "        ax.plot(dataset_df['_time'], dataset_df['value'], color='black', label=f'{dataset_name}')\n",
    "    else:\n",
    "        detection_query = f'''\n",
    "        from(bucket: \"{bucket_ano}\")\n",
    "            |> range(start: time(v: \"{start_time}\"))\n",
    "            |> filter(fn: (r) => r[\"_measurement\"] == \"{measurements[i-1]}\")\n",
    "            |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "        '''\n",
    "\n",
    "        marker_df = query_api.query_data_frame(query=detection_query)\n",
    "        binary_series = pd.Series(0, index=dataset_df['_time'])\n",
    "        binary_series[marker_df['_time']] = 1\n",
    "        ax.step(binary_series.index, binary_series, where='post', label=names[i-1], color='black')\n",
    "\n",
    "# Create an overlay axis for drawing the labels\n",
    "overlay_ax = fig.add_subplot(111, label=\"overlay\", frame_on=False)\n",
    "overlay_ax.set_xticks([])\n",
    "overlay_ax.set_yticks([])\n",
    "overlay_ax.set_xlim(axs[0].get_xlim())\n",
    "overlay_ax.set_ylim(0, 1)\n",
    "\n",
    "# Labels query and draw lines across all subplots (overlay)\n",
    "labels_query = f'''\n",
    "from(bucket: \"{bucket}\")\n",
    "    |> range(start: time(v: \"{start_time}\"))\n",
    "    |> filter(fn: (r) => r[\"_measurement\"] == \"{dataset_labels}\")\n",
    "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "'''\n",
    "labels_df = query_api.query_data_frame(query=labels_query)\n",
    "\n",
    "# Result query for grouping the anomalies\n",
    "result_query = f'''\n",
    "from(bucket: \"{bucket_ano}\")\n",
    "    |> range(start: time(v: \"{start_time}\"))\n",
    "    |> filter(fn: (r) => r[\"_measurement\"] == \"{dataset_results}\")\n",
    "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "'''\n",
    "result_df = query_api.query_data_frame(query=result_query)\n",
    "\n",
    "# Merge the labels and results\n",
    "merged_labels = pd.merge(result_df, labels_df, on='_time', how='inner')\n",
    "# Create groups based on neighbouring T-values\n",
    "merged_labels['group'] = (merged_labels['T'].diff() != 1).cumsum()\n",
    "\n",
    "# Draw the anomalies on the overlay axis\n",
    "if not merged_labels.empty:\n",
    "    for group, group_df in merged_labels.groupby('group'):\n",
    "        start = group_df['_time'].iloc[0]\n",
    "        end = group_df['_time'].iloc[-1]\n",
    "        overlay_ax.axvspan(start, end, color='red', alpha=0.45, lw=2)\n",
    "\n",
    "# Configure plot settings\n",
    "for ax in axs:\n",
    "    leg = ax.legend(loc='lower right', fontsize='small', handlelength=0, handletextpad=0)\n",
    "    leg.legend_handles[0].set_visible(False)\n",
    "    ax.grid(True)\n",
    "\n",
    "axs[-1].set_xlabel('Timestamp')\n",
    "plt.grid(True)\n",
    "plt.tight_layout() # Avoids overlapping labels with legend, \n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig(f'{dataset_labels}.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Older and alternative way of plotting the data as script above\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from influxdb_client import InfluxDBClient\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# Settings\n",
    "influxdb_url = \"http://localhost:8086\"\n",
    "token = \"random_token\"\n",
    "org = \"ORG\"\n",
    "bucket = \"system_state\"\n",
    "dataset = \"CC2\"\n",
    "bucket_ano = \"anomalies\"\n",
    "dataset_labels = \"labels_CC2\"\n",
    "start_time = \"1997-04-10T00:00:00Z\"\n",
    "\n",
    "measurements = [\n",
    "    \"repad2-cc2-detection-minmax\",\n",
    "    \"repad2-cc2-detection-robust\",\n",
    "    \"repad2-cc2-detection-standard\",\n",
    "    \"miura-cc2-detection-minmax\",\n",
    "    \"miura-cc2-detection-robust\",\n",
    "    \"miura-cc2-detection-standard\",\n",
    "]\n",
    "\n",
    "# Instantiate the InfluxDB client\n",
    "client = InfluxDBClient(url=influxdb_url, token=token, org=org)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Main dataset plot\n",
    "dataset_query = f'''\n",
    "from(bucket: \"{bucket}\")\n",
    "    |> range(start: time(v: \"{start_time}\"))\n",
    "    |> filter(fn: (r) => r[\"_measurement\"] == \"{dataset}\")\n",
    "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "'''\n",
    "dataset_df = query_api.query_data_frame(query=dataset_query)\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(len(measurements) + 1, 1, figsize=(12, 8), sharex=True)\n",
    "axs[0].plot(dataset_df['_time'], dataset_df['value'], label='Value', color='black')\n",
    "\n",
    "# Overlay subplot for labels\n",
    "#fig.subplots_adjust(hspace=0)  # Adjust horizontal space to minimize gaps between subplots\n",
    "overlay_ax = fig.add_subplot(111, label=\"overlay\", zorder=-1)\n",
    "overlay_ax.set_frame_on(False)  # Turn off frame to not cover other plots\n",
    "overlay_ax.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "\n",
    "# Plot each measurement as binary series\n",
    "for i, measurement in enumerate(measurements, start=1):\n",
    "    detection_query = f'''\n",
    "    from(bucket: \"{bucket_ano}\")\n",
    "        |> range(start: time(v: \"{start_time}\"))\n",
    "        |> filter(fn: (r) => r[\"_measurement\"] == \"{measurement}\")\n",
    "        |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "    '''\n",
    "    marker_df = query_api.query_data_frame(query=detection_query)\n",
    "    # Create a series of zeros\n",
    "    binary_series = pd.Series(0, index=dataset_df['_time'])\n",
    "    # Set detected anomalies to 1\n",
    "    binary_series[marker_df['_time']] = 1\n",
    "    axs[i].step(binary_series.index, binary_series, where='post', label=measurement, color='black')\n",
    "\n",
    " # Labels query and draw lines across all subplots\n",
    "labels_query = f'''\n",
    "from(bucket: \"{bucket}\")\n",
    "    |> range(start: time(v: \"{start_time}\"))\n",
    "    |> filter(fn: (r) => r[\"_measurement\"] == \"{dataset_labels}\")\n",
    "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "'''\n",
    "labels_df = query_api.query_data_frame(query=labels_query)\n",
    "# Plot labels across all subplots\n",
    "if not labels_df.empty:\n",
    "    for timestamp in labels_df['_time']:\n",
    "        for ax in axs:\n",
    "            ax.axvline(x=timestamp, color='red', linestyle=':', linewidth=2, clip_on=False, label='Label' if 'Label' not in ax.get_legend_handles_labels()[1] else \"\") \n",
    "\n",
    "# Configure plot settings\n",
    "for ax in axs:\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "#plt.legend()\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "plt.gca().margins(x=0)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
