{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Standard_LSTM_inc_model import LSTM_inc\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from collections import deque\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import ASYNCHRONOUS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seed for reproducibility\n",
    "torch.manual_seed(140)\n",
    "np.random.seed(140)\n",
    "random.seed(140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for RePAD2\n",
    "def calculate_aare(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the Absolute Relative Error (ARE) between an actual and predicted value.\n",
    "    \n",
    "    Parameters:\n",
    "    actual (deque): The actual value.\n",
    "    predicted (deque): The predicted value.\n",
    "    \n",
    "    Returns:\n",
    "    float: The Absolute Relative Error.\n",
    "    \"\"\"\n",
    "    # Adding a small value epsilon to avoid division by zero\n",
    "    aare_values = []\n",
    "\n",
    "    # To test with only a single value of actual and predicted to calculate AARE\n",
    "    #actual = actual[-1]\n",
    "    #predicted = predicted[-1]\n",
    "    \n",
    "    for act, pred in zip(actual, predicted):\n",
    "        AARE = abs(act - pred) / max(abs(act), 1)\n",
    "        aare_values.append(AARE)\n",
    "\n",
    "    mean_aare = np.mean(aare_values)\n",
    "\n",
    "    return mean_aare\n",
    "\n",
    "\n",
    "def calculate_threshold(aare_values):\n",
    "    \"\"\"\n",
    "    Calculate the threshold value (Thd) based on a deque of AARE values.\n",
    "    Thd is defined as the mean of the AARE values plus three times their standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - aare_values (array-like): An array of AARE values.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated threshold value (Thd).\n",
    "    \"\"\"\n",
    "    # Calculate the mean and standard deviation of the AARE values\n",
    "    mean_aare = np.mean(aare_values)\n",
    "    std_aare = np.std(aare_values)\n",
    "    \n",
    "    # Calculate Thd\n",
    "    thd = mean_aare + 3 * std_aare\n",
    "    \n",
    "    return thd\n",
    "\n",
    "# Function for initializing the model\n",
    "def initialize(train_events):\n",
    "    tensor_y = torch.tensor(train_events, dtype=torch.float32).view(-1, 1, 1)\n",
    "    tensor_x = torch.tensor([1, 2, 3], dtype=torch.float32).view(-1, 1, 1)\n",
    "    # Create an instance of the LSTM model\n",
    "    model = LSTM_inc(input_size=1, hidden_size=10, num_layers=1, output_size=1, num_epochs=50, learning_rate=0.005)\n",
    "\n",
    "    model.initialize(tensor_x, tensor_y) # Train the model\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for incrementally updating the model\n",
    "def update_model(model, train_events):\n",
    "    tensor_y = torch.tensor(train_events, dtype=torch.float32).view(-1, 1, 1)\n",
    "    tensor_x = torch.tensor([1, 2, 3], dtype=torch.float32).view(-1, 1, 1)\n",
    "    model.continue_training(tensor_x, tensor_y) # Continue training the model\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for reporting anomalies to InfluxDB\n",
    "def report_anomaly(T, timestamp, actual_value, predicted_value, write_api):\n",
    "    \"\"\"\n",
    "    Sends an anomalous event back to InfluxDB, storing it in the \"anomaly\" measurement\n",
    "    with both the same value and time as the original event.\n",
    "\n",
    "    Parameters:\n",
    "    - anomalous_event: The event data that was detected as an anomaly, including its value and timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    point = Point(\"miura-detection-cc2-standard\")\\\n",
    "        .tag(\"host\", \"detector\")\\\n",
    "        .field(\"T\", float(T))\\\n",
    "        .field(\"actual_value\", float(actual_value))\\\n",
    "        .field(\"predicted_value\", float(predicted_value))\\\n",
    "        .time(timestamp, WritePrecision.NS)\n",
    "    \n",
    "    write_api.write(bucket=\"anomalies\", org=\"ORG\", record=point)\n",
    "    #print(f\"Anomalous event sent to InfluxDB: Value={value}, Time={timestamp}\")\n",
    "\n",
    "def write_result(timestamp, T, actual_value, predicted_value, AARE, Thd, write_api):\n",
    "    \"\"\"\n",
    "    Sends an anomalous event back to InfluxDB, storing it in the \"anomaly\" measurement\n",
    "    with both the same value and time as the original event.\n",
    "\n",
    "    Parameters:\n",
    "    - anomalous_event: The event data that was detected as an anomaly, including its value and timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    point = Point(\"miura-result-cc2-standard\")\\\n",
    "        .tag(\"host\", \"detector\")\\\n",
    "        .field(\"T\", float(T))\\\n",
    "        .field(\"actual_value\", float(actual_value))\\\n",
    "        .field(\"predicted_value\", float(predicted_value))\\\n",
    "        .field(\"AARE\", float(AARE))\\\n",
    "        .field(\"Thd\", float(Thd))\\\n",
    "        .time(timestamp, WritePrecision.NS)\n",
    "    \n",
    "    #write_api.write(bucket=\"anomalies\", org=\"ORG\", record=point)\n",
    "    #print(f'T: {T}, Real Value: {actual_value}, Prediction Value: {predicted_value}, AARE: {AARE}, Thd: {Thd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPAD2 Algorithm ###\n",
    "\n",
    "\"\"\"\n",
    "THE PLAN\n",
    "\n",
    "Making the program start from any predefined time, and then continue to fetch data from that time and onwards.\n",
    "And so it can process older data and then catch up to the present time.\n",
    "\n",
    "It therefore processes a batch of 3 from the earliest timestamp in the range, and then updates the start time for the next iteration \n",
    "by incrementing the start time by a set time to both avoid duplicate events and to eventually catch up to the present time.\n",
    "\n",
    "The program will run indefinitely, and will continue to fetch data from the InfluxDB and process it in batches of 3 until the program is stopped.\n",
    "I there are not enough events for a batch, the program will wait for a set time before trying again.\n",
    "When a batch of three is available it will be processed and the it will again wait for another event to be available.\n",
    "\n",
    "This way it is both flexible and efficient, and can be easily used to process either data in real-time or historical data\n",
    "\n",
    "\n",
    "For each batch it follows the algorithm of RePAD2\n",
    "\n",
    "\n",
    "To-Do:\n",
    "\n",
    "Store actual values (Store whole event?) with predicted values in sliding window \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Testing of InfluxDB with LSTM\n",
    "# MÃ¥ installeres influxdb-client \"pip install influxdb-client\"\n",
    "# https://www.influxdata.com/blog/getting-started-with-python-and-influxdb-v2-0/\n",
    "influxdb_url = \"http://localhost:8086\"\n",
    "token = \"random_token\"\n",
    "username = \"influx-admin\"\n",
    "password = \"ThisIsNotThePasswordYouAreLookingFor\"\n",
    "org = \"ORG\"\n",
    "bucket = \"system_state\"\n",
    "measurement = \"CC2\"\n",
    "\n",
    "# Instantiate the QueryAPI\n",
    "client = InfluxDBClient(url=influxdb_url, token=token, org=org, username=username, password=password)\n",
    "write_api = client.write_api(write_options=ASYNCHRONOUS)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Sliding window for Threshold\n",
    "actual_value = deque([0] * 3, maxlen=3)\n",
    "predicted_value = deque([0] * 3, maxlen=3)\n",
    "sliding_window_AARE = deque(maxlen=8064)\n",
    "\n",
    "# Other data structures\n",
    "batch_events = deque(maxlen=3)\n",
    "next_event = None\n",
    "\n",
    "# For printing the values\n",
    "AARE_T = 0\n",
    "Thd = 0\n",
    "\n",
    "# Time parameters\n",
    "poll_interval = 1  # Second(s)\n",
    "time_increment = 1 # Second(s)\n",
    "start_time = \"1997-04-10T00:00:00Z\"\n",
    "\n",
    "# RePAD2 specific\n",
    "T = 0\n",
    "flag = True\n",
    "M = None # Model\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "all_times = []\n",
    "#model_training_times = []\n",
    "under_threshold_times = []\n",
    "non_anomaly_times = []\n",
    "anomaly_times = []\n",
    "model_event_count = 0\n",
    "model_train_count = 0\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Construct the Flux query\n",
    "    query = f'''\n",
    "    from(bucket: \"{bucket}\")\n",
    "     |> range(start: time(v: \"{start_time}\"))\n",
    "     |> filter(fn: (r) => r[\"_measurement\"] == \"{measurement}\")\n",
    "    '''\n",
    "        \n",
    "    # Query the data\n",
    "    events = list(query_api.query_stream(org=org, query=query))\n",
    "\n",
    "    if len(events) > 1: # Need at least 4 to predict next and compar\n",
    "\n",
    "        for i in range(len(events)-1):\n",
    "\n",
    "            start_processing_time = time.time() # EVAL - Start timing for processing the batch\n",
    "\n",
    "            batch_events.append(events[i])\n",
    "            next_event = events[i+1]     \n",
    "\n",
    "        # RePAD2 Algorithm\n",
    "\n",
    "        # Set T to the length of the sliding \n",
    "            if T >= 2 and T < 5: # 3 total values when T = 2\n",
    "                if T == 2:\n",
    "                    # Initialize the model\n",
    "                    M = initialize([event.get_value() for event in list(batch_events)])\n",
    "                else: \n",
    "                    M = update_model(M, [event.get_value() for event in list(batch_events)])\n",
    "                pred_D_T_plus_1 = M.predict_next()\n",
    "                # Append the event and its prediction to the sliding window\n",
    "                actual_value.append(next_event.get_value())\n",
    "                predicted_value.append(pred_D_T_plus_1)\n",
    "\n",
    "            elif T >= 5 and T < 7:\n",
    "                # Calculate AARE and append to sliding window\n",
    "                AARE_T = calculate_aare(actual_value, predicted_value)\n",
    "                sliding_window_AARE.append(AARE_T)\n",
    "                M = update_model(M, [event.get_value() for event in list(batch_events)])\n",
    "                pred_D_T_plus_1 = M.predict_next()\n",
    "                # Append the event and its prediction to the sliding window\n",
    "                actual_value.append(next_event.get_value())\n",
    "                predicted_value.append(pred_D_T_plus_1)\n",
    "\n",
    "            elif T >= 7 and flag == True:\n",
    "                if T != 7: # Use M to precdict D_T\n",
    "                    pred_D_T = M.predict_next()\n",
    "                    # Append the event and its prediction to the sliding window\n",
    "                    actual_value.append(next_event.get_value())\n",
    "                    predicted_value.append(pred_D_T)\n",
    "\n",
    "                # Calculate AARE and append to sliding window\n",
    "                AARE_T = calculate_aare(actual_value, predicted_value)\n",
    "                sliding_window_AARE.append(AARE_T)\n",
    "                # Calculate Thd\n",
    "                Thd = calculate_threshold(sliding_window_AARE)\n",
    "                \n",
    "                if AARE_T <= Thd: pass # Calculate AARE and append to sliding window\n",
    "                else:\n",
    "                    # Train an LSTM model with D_T-3, D_T-2, D_T-1\n",
    "                    model = update_model(M, [event.get_value() for event in list(batch_events)])\n",
    "                    # Use the model to predict D_T\n",
    "                    pred_D_T = model.predict_next()\n",
    "                    \n",
    "                    # Append the event and its prediction to the sliding window\n",
    "                    actual_value.append(next_event.get_value())\n",
    "                    predicted_value.append(pred_D_T)\n",
    "\n",
    "                    # Re-calculate AARE_T\n",
    "                    AARE_T = calculate_aare(actual_value, predicted_value)\n",
    "                    sliding_window_AARE.append(AARE_T)\n",
    "                    # Re-calculate Thd\n",
    "                    Thd = calculate_threshold(sliding_window_AARE)\n",
    "\n",
    "                    model_train_count += 1 # EVAL - Increment the model training count\n",
    "\n",
    "                    if AARE_T <= Thd:\n",
    "                        # D_T is not reported as anomaly\n",
    "                        # Replace M with the new model\n",
    "                        M = model\n",
    "                        # Update flag to True\n",
    "                        flag = True\n",
    "\n",
    "                        # EVAL - Append the time to the list of non-anomaly times\n",
    "                        non_anomaly_times.append(time.time() - start_processing_time)\n",
    "                    else:\n",
    "                        # D_T reported as anomaly immediately\n",
    "                        report_anomaly(T+1, next_event.get_time(), actual_value[-1], predicted_value[-1], write_api)\n",
    "                        # Update flag to False\n",
    "                        flag = False\n",
    "\n",
    "                        # EVAL - Append the time to list of anomaly times\n",
    "                        anomaly_times.append(time.time() - start_processing_time)\n",
    "            \n",
    "            elif T >= 7 and flag == False:\n",
    "                # Train an LSTM model with D_T-3, D_T-2, D_T-1\n",
    "                model = update_model(M, [event.get_value() for event in batch_events])\n",
    "                # Use the model to predict D_T\n",
    "                pred_D_T = model.predict_next()\n",
    "                # Append the event and its prediction to the sliding window\n",
    "                actual_value.append(next_event.get_value())\n",
    "                predicted_value.append(pred_D_T)\n",
    "\n",
    "                # Calculate AARE_T\n",
    "                AARE_T = calculate_aare(actual_value, predicted_value)\n",
    "                sliding_window_AARE.append(AARE_T)\n",
    "                # Calculate Thd\n",
    "                Thd = calculate_threshold(sliding_window_AARE)\n",
    "\n",
    "                if AARE_T <= Thd:\n",
    "                    # D_T is not reported as anomaly\n",
    "                    # Replace M with the new model\n",
    "                    M = model\n",
    "                    # Update flag to True\n",
    "                    flag = True\n",
    "\n",
    "                    # EVAL - Append the time to the list of non-anomaly times\n",
    "                    non_anomaly_times.append(time.time() - start_processing_time)\n",
    "\n",
    "                else:\n",
    "                    # D_T reported as anomaly immediately\n",
    "                    report_anomaly(T+1, next_event.get_time(), actual_value[-1], predicted_value[-1], write_api)\n",
    "                    # Update flag to False\n",
    "                    flag = False\n",
    "\n",
    "                    # EVAL - Append the time to list of anomaly times\n",
    "                    anomaly_times.append(time.time() - start_processing_time)\n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            process_time = end_time - start_processing_time # EVAL - Time to process the batch\n",
    "            all_times.append(process_time) # EVAL - Append the time to the list of all times\n",
    "\n",
    "\n",
    "            if AARE_T <= Thd: # EVAL - Add to the list of times under the threshold\n",
    "                under_threshold_times.append(process_time) \n",
    "\n",
    "            # Write the results to InfluxDB\n",
    "            write_result(batch_events[-1].get_time(), T, batch_events[-1].get_value(), predicted_value[-2], AARE_T, Thd, write_api) \n",
    "\n",
    "            # Increment T\n",
    "            T += 1\n",
    "\n",
    "        # Update start time for the next iteration\n",
    "        last_event_time = batch_events[-1].get_time()\n",
    "        # Increment by 1 second to avoid duplicate events\n",
    "        start_time = (last_event_time + timedelta(seconds=time_increment)).isoformat()\n",
    "                \n",
    "\n",
    "    else:\n",
    "        print(\"No events found in range.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(poll_interval)\n",
    "\n",
    "    # Star the program and let it run indefinitely, then try to send and even bigger dataset to the same measurements and see if it can handle itdenz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process evaluation data\n",
    "\n",
    "# Use to not run cell when \"Run All\"\n",
    "#raise SystemExit(\"Stop right here!\")\n",
    "\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Average time consumption for processing each batch X\n",
    "average_batch_time = sum(all_times) / len(all_times) if all_times else 0\n",
    "std_batch_time = np.std(all_times) if all_times else 0 \n",
    "\n",
    "# Average time consumption when AARE <= Thd and standard deviation X\n",
    "average_under_thd_time = sum(under_threshold_times) / len(under_threshold_times) if under_threshold_times else 0\n",
    "std_under_thd_time = np.std(under_threshold_times) if under_threshold_times else 0\n",
    "\n",
    "# Average time consumption when AARE > Thd and is not reported as anomaly and standard deviation X\n",
    "average_non_anomaly_time = sum(non_anomaly_times) / len(non_anomaly_times) if non_anomaly_times else 0\n",
    "std_non_anomaly_time = np.std(non_anomaly_times) if non_anomaly_times else 0\n",
    "\n",
    "# Average time consumption when AARE > Thd and is reported as anomaly and standard deviation X\n",
    "average_anomaly_time = sum(anomaly_times) / len(anomaly_times) if anomaly_times else 0\n",
    "std_anomaly_time = np.std(anomaly_times) if anomaly_times else 0\n",
    "\n",
    "# Average LSTM model training time\n",
    "#average_model_training_time = sum(model_training_times) / len(model_training_times) if model_training_times else 0\n",
    "#std_model_training_time = np.std(model_training_times) if model_training_times else 0\n",
    "\n",
    "# LSTM model retraining ratio\n",
    "retraining_ratio = model_train_count / (len(all_times) + model_train_count) if (len(all_times) + model_train_count) else 0\n",
    "\n",
    "\n",
    "# Create a DataFrame to organize these results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Average (sec)\": [\n",
    "        average_batch_time,\n",
    "        average_under_thd_time, \n",
    "        average_non_anomaly_time, \n",
    "        average_anomaly_time, \n",
    "        retraining_ratio\n",
    "    ],\n",
    "    \"Standard Deviation (sec)\": [\n",
    "        std_batch_time,\n",
    "        std_under_thd_time, \n",
    "        std_non_anomaly_time, \n",
    "        std_anomaly_time, \n",
    "        np.nan  # Not applicable for the ratio\n",
    "    ]\n",
    "}, index=[\n",
    "    \"Average Batch Processing Time\",\n",
    "    \"Average Time when AARE <= Thd\",\n",
    "    \"Average Time when AARE > Thd (Non-anomaly)\",\n",
    "    \"Average Time when AARE > Thd (Anomaly)\",\n",
    "    \"LSTM Model Retraining Ratio\"\n",
    "])\n",
    "\n",
    "#results_df\n",
    "\n",
    "results_md = tabulate(results_df, headers='keys', tablefmt='pipe')\n",
    "\n",
    "with open(f'../Timing/{measurement}_miura_standard', 'w') as f:\n",
    "    f.write(results_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
