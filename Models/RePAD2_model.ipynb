{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM_model import LSTM\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seed for reproducibility\n",
    "torch.manual_seed(140)\n",
    "np.random.seed(140)\n",
    "random.seed(140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of LSTM-class\n",
    "x_values = torch.tensor([1, 2, 3], dtype=torch.float32).view(-1, 1, 1)\n",
    "y_values = torch.tensor([10, 20, 30], dtype=torch.float32).view(-1, 1, 1)\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "model = LSTM(x_values, y_values, input_size=1, hidden_size=10, num_layers=1, output_size=1, batch_size=3, num_epochs=50, learning_rate=0.01)\n",
    "model.train_model() # Train the model\n",
    "\n",
    "# Get a prediction\n",
    "x_test = torch.tensor([4], dtype=torch.float32).view(-1, 1, 1)\n",
    "x_test = model.transform(x_test)\n",
    "y_pred = model(x_test)\n",
    "y_pred_orig = model.inverse_transform_y(y_pred)\n",
    "\n",
    "print(f'Prediction: {y_pred_orig.item()}')  # Prediction: 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of InfluxDB\n",
    "# Må installeres influxdb-client \"pip install influxdb-client\"\n",
    "# https://www.influxdata.com/blog/getting-started-with-python-and-influxdb-v2-0/\n",
    "influxdb_url = \"http://localhost:8086\"\n",
    "token = \"random_token\"\n",
    "username = \"influx-admin\"\n",
    "password = \"ThisIsNotThePasswordYouAreLookingFor\"\n",
    "org = \"ORG\"\n",
    "bucket = \"system_state\"\n",
    "\n",
    "client = InfluxDBClient(url=influxdb_url, token=token, org=org, username=username, password=password)\n",
    "\n",
    "poll_interval = 1\n",
    "\n",
    "# Instantiate the QueryAPI\n",
    "query_api = client.query_api()\n",
    "\n",
    "batch_nr = 0\n",
    "\n",
    "# Initialize timestamp\n",
    "# Ensure last_timestamp is a datetime object for accurate comparison\n",
    "last_timestamp = datetime.fromisoformat(\"1970-01-01T00:00:00+00:00\")\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Construct the Flux query\n",
    "    query = f'''\n",
    "    from(bucket: \"{bucket}\")\n",
    "     |> range(start: time(v: \"2014-04-10T00:00:00Z\"))\n",
    "     |> filter(fn: (r) => r[\"_measurement\"] == \"cpu_utilization\")\n",
    "     |> tail(n: 3)\n",
    "    '''\n",
    "            \n",
    "\n",
    "    # Query the data\n",
    "    events = list(query_api.query_stream(org=org, query=query))\n",
    "\n",
    "    \n",
    "\n",
    "    if events:\n",
    "        # Extract the timestamp of the last event\n",
    "        new_last_event = events[-1]\n",
    "        new_last_timestamp = new_last_event.get_time()  # This should already be a datetime object\n",
    "\n",
    "        # Compare datetime objects directly\n",
    "        if new_last_timestamp > last_timestamp:\n",
    "            # Increment batch_nr\n",
    "            batch_nr += 1\n",
    "            # Print batch nr as seperator\n",
    "            print(\"Batch nr: \", batch_nr)\n",
    "\n",
    "            # New event detected\n",
    "            for event in events:\n",
    "                # Print the timestamp and value of the event\n",
    "                print(f'Time: {event.get_time()}, Value: {event.get_value()}')\n",
    "\n",
    "            # Update last_timestamp for the next iteration, converting back to ISO format if necessary\n",
    "            last_timestamp = new_last_timestamp \n",
    "\n",
    "       # else:\n",
    "            #print(\"No new events found.\")\n",
    "    else:\n",
    "        print(\"No events found in range.\")\n",
    "\n",
    "    time.sleep(poll_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further testing of InfluxDB\n",
    "\"\"\"\n",
    "THE PLAN\n",
    "\n",
    "Making the program start from any predefined time, and then continue to fetch data from that time and onwards.\n",
    "And so it can process older data and then catch up to the present time.\n",
    "\n",
    "It therefore processes a batch of 3 from the earliest timestamp in the range, and then updates the start time for the next iteration \n",
    "by incrementing the start time by a set time to both avoid duplicate events and to eventually catch up to the present time.\n",
    "\n",
    "The program will run indefinitely, and will continue to fetch data from the InfluxDB and process it in batches of 3 until the program is stopped.\n",
    "I there are not enough events for a batch, the program will wait for a set time before trying again.\n",
    "When a batch of three is available it will be processed and the it will again wait for another event to be available.\n",
    "\n",
    "This way it is both flexible and efficient, and can be easily used to process either data in real-time or historical data\n",
    "\n",
    "\n",
    "For each batch it follows the algorithm of RePAD2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Testing of InfluxDB\n",
    "# Må installeres influxdb-client \"pip install influxdb-client\"\n",
    "# https://www.influxdata.com/blog/getting-started-with-python-and-influxdb-v2-0/\n",
    "influxdb_url = \"http://localhost:8086\"\n",
    "token = \"random_token\"\n",
    "username = \"influx-admin\"\n",
    "password = \"ThisIsNotThePasswordYouAreLookingFor\"\n",
    "org = \"ORG\"\n",
    "bucket = \"system_state\"\n",
    "\n",
    "# Instantiate the QueryAPI\n",
    "client = InfluxDBClient(url=influxdb_url, token=token, org=org, username=username, password=password)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Sliding window for Threshold\n",
    "sliding_window = deque(maxlen=8064)\n",
    "\n",
    "# Time parameters\n",
    "poll_interval = 1\n",
    "time_increment = 1\n",
    "start_time = \"2014-04-10T00:00:00Z\"\n",
    "\n",
    "is_first_batch = True\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Construct the Flux query\n",
    "    query = f'''\n",
    "    from(bucket: \"{bucket}\")\n",
    "     |> range(start: time(v: \"{start_time}\"))\n",
    "     |> filter(fn: (r) => r[\"_measurement\"] == \"cpu_utilization\")\n",
    "    '''\n",
    "        \n",
    "    # Query the data\n",
    "    events = list(query_api.query_stream(org=org, query=query))\n",
    "\n",
    "    if events:\n",
    "        # Process batches of 3 until there are not enough events for a batch\n",
    "        for i in range(len(events) - 2):\n",
    "            # Select the batch of 3 events\n",
    "            batch_events = events[i:i+3]\n",
    "\n",
    "            if is_first_batch:\n",
    "                for event in batch_events:\n",
    "                    sliding_window.append(event) # Storing all events in batch\n",
    "                is_first_batch = False\n",
    "\n",
    "            else:\n",
    "                # For subsequent batches, append only the last event\n",
    "                sliding_window.append(batch_events[-1])\n",
    "\n",
    "            # After processing the event\n",
    "            if i + 3 >= len(events):\n",
    "                # Update start time for the next iteration\n",
    "                last_event_time = batch_events[-1].get_time()\n",
    "                # Increment by 1 second to avoid duplicate events\n",
    "                start_time = (last_event_time + timedelta(seconds=time_increment)).isoformat()\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        print(\"No events found in range.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(poll_interval)\n",
    "\n",
    "# Print a padding line\n",
    "#print(\"Padding line\")\n",
    "for event in sliding_window:\n",
    "    print(f'Time: {event.get_time()}, Value: {event.get_value()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for RePAD2\n",
    "def calculate_aare(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the Average Absolute Relative Error (AARE) between actual and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - actual (list or array): The actual values.\n",
    "    - predicted (list or array): The predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - float: The AARE value.\n",
    "    \"\"\"\n",
    "    if len(actual) != len(predicted):\n",
    "        raise ValueError(\"The length of actual and predicted values must be the same.\")\n",
    "\n",
    "    # Calculate the absolute relative errors\n",
    "    absolute_relative_errors = [abs((a - p) / a) for a, p in zip(actual, predicted) if a != 0]\n",
    "\n",
    "    # Calculate the average of these errors\n",
    "    aare = sum(absolute_relative_errors) / len(absolute_relative_errors)\n",
    "    \n",
    "    return aare\n",
    "\n",
    "# Example usage:\n",
    "actual_values = [100, 200, 300, 400]\n",
    "predicted_values = [90, 210, 310, 390]\n",
    "aare_result = calculate_aare(actual_values, predicted_values)\n",
    "print(f\"AARE: {aare_result}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_threshold(aare_values):\n",
    "    \"\"\"\n",
    "    Calculate the threshold value (Thd) based on an array of AARE values.\n",
    "    Thd is defined as the mean of the AARE values plus three times their standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - aare_values (array-like): An array of AARE values.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated threshold value (Thd).\n",
    "    \"\"\"\n",
    "    # Calculate the mean and standard deviation of the AARE values\n",
    "    mean_aare = np.mean(aare_values)\n",
    "    std_aare = np.std(aare_values)\n",
    "    \n",
    "    # Calculate Thd\n",
    "    thd = mean_aare + 3 * std_aare\n",
    "    \n",
    "    return thd\n",
    "\n",
    "# Example usage\n",
    "aare_values = [0.05, 0.07, 0.06, 0.08, 0.09]\n",
    "thd = calculate_threshold(aare_values)\n",
    "print(f\"Threshold (Thd): {thd}\")\n",
    "\n",
    "# Function for creating model\n",
    "def train_model(train_events):\n",
    "    tensor_y = torch.tensor(train_events, dtype=torch.float32).view(-1, 1, 1)\n",
    "    tensor_x = torch.tensor([1, 2, 3], dtype=torch.float32).view(-1, 1, 1)\n",
    "    # Create an instance of the LSTM model\n",
    "    model = LSTM(tensor_x, tensor_y, input_size=1, hidden_size=10, num_layers=1, output_size=1, batch_size=3, num_epochs=50, learning_rate=0.01)\n",
    "    model.train_model() # Train the model\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def report_anomaly(anomalous_event, write_api):\n",
    "    \"\"\"\n",
    "    Sends an anomalous event back to InfluxDB, storing it in the \"anomaly\" measurement\n",
    "    with both the same value and time as the original event.\n",
    "\n",
    "    Parameters:\n",
    "    - anomalous_event: The event data that was detected as an anomaly, including its value and timestamp.\n",
    "    \"\"\"\n",
    "    point = Point(\"anomaly\")\\\n",
    "        .tag(\"host\", \"host1\")\\\n",
    "        .field(\"value\", anomalous_event.value)\\\n",
    "        .time(anomalous_event.timestamp, WritePrecision.NS)\n",
    "    \n",
    "    write_api.write(bucket=\"system_state\", org=\"ORG\", record=point)\n",
    "    print(f\"Anomalous event sent to InfluxDB: Value={anomalous_event.value}, Time={anomalous_event.timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of InfluxDB with LSTM\n",
    "\"\"\"\n",
    "THE PLAN\n",
    "\n",
    "Making the program start from any predefined time, and then continue to fetch data from that time and onwards.\n",
    "And so it can process older data and then catch up to the present time.\n",
    "\n",
    "It therefore processes a batch of 3 from the earliest timestamp in the range, and then updates the start time for the next iteration \n",
    "by incrementing the start time by a set time to both avoid duplicate events and to eventually catch up to the present time.\n",
    "\n",
    "The program will run indefinitely, and will continue to fetch data from the InfluxDB and process it in batches of 3 until the program is stopped.\n",
    "I there are not enough events for a batch, the program will wait for a set time before trying again.\n",
    "When a batch of three is available it will be processed and the it will again wait for another event to be available.\n",
    "\n",
    "This way it is both flexible and efficient, and can be easily used to process either data in real-time or historical data\n",
    "\n",
    "\n",
    "For each batch it follows the algorithm of RePAD2\n",
    "\n",
    "\n",
    "To-Do:\n",
    "\n",
    "Store actual values (Store whole event?) with predicted values in sliding window \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Testing of InfluxDB with LSTM\n",
    "# Må installeres influxdb-client \"pip install influxdb-client\"\n",
    "# https://www.influxdata.com/blog/getting-started-with-python-and-influxdb-v2-0/\n",
    "influxdb_url = \"http://localhost:8086\"\n",
    "token = \"random_token\"\n",
    "username = \"influx-admin\"\n",
    "password = \"ThisIsNotThePasswordYouAreLookingFor\"\n",
    "org = \"ORG\"\n",
    "bucket = \"system_state\"\n",
    "\n",
    "# Instantiate the QueryAPI\n",
    "client = InfluxDBClient(url=influxdb_url, token=token, org=org, username=username, password=password)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Sliding window for Threshold\n",
    "sliding_window = deque(maxlen=8064)\n",
    "sliding_window_AARE = deque(maxlen=8064)\n",
    "\n",
    "# Time parameters\n",
    "poll_interval = 1\n",
    "time_increment = 1\n",
    "start_time = \"2014-04-10T00:00:00Z\"\n",
    "\n",
    "batch_nr = 0\n",
    "is_first_batch = True\n",
    "\n",
    "# RePAD2 specific\n",
    "flag = True\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Construct the Flux query\n",
    "    query = f'''\n",
    "    from(bucket: \"{bucket}\")\n",
    "     |> range(start: time(v: \"{start_time}\"))\n",
    "     |> filter(fn: (r) => r[\"_measurement\"] == \"cpu_utilization\")\n",
    "    '''\n",
    "        \n",
    "    # Query the data\n",
    "    events = list(query_api.query_stream(org=org, query=query))\n",
    "\n",
    "    if len(events) >= 4: # Need at least 4 to predict next and compare\n",
    "        for i in range(len(events) - 3):\n",
    "            batch_events = events[i:i+3]\n",
    "            next_event = events[i+3]\n",
    "\n",
    "\n",
    "    if len(events) >= 3:\n",
    "        # Process batches of 3 until there are not enough events for a batch\n",
    "        for i in range(len(events) - 2):\n",
    "            # Select the batch of 3 events\n",
    "            batch_events = events[i:i+3]\n",
    "\n",
    "            if is_first_batch:\n",
    "                for event in batch_events:\n",
    "                    sliding_window.append(event) # Storing all events in batch\n",
    "                is_first_batch = False\n",
    "\n",
    "            else:\n",
    "                # For subsequent batches, append only the last event\n",
    "                sliding_window.append(batch_events[-1])\n",
    "\n",
    "################# LSTM Testing ############################\n",
    "\n",
    "            if len(sliding_window) >= 3:\n",
    "                model = train_model([event.get_value() for event in list(batch_events)])\n",
    "                pred = model.predict_next()\n",
    "                print(f'Prediction: {pred}')\n",
    "\n",
    "                \n",
    "\n",
    "################# LSTM Testing ############################\n",
    "\n",
    "            # After processing the event\n",
    "            if i + 3 >= len(events):\n",
    "                # Update start time for the next iteration\n",
    "                last_event_time = batch_events[-1].get_time()\n",
    "                # Increment by 1 second to avoid duplicate events\n",
    "                start_time = (last_event_time + timedelta(seconds=time_increment)).isoformat()\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        print(\"No events found in range.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(poll_interval)\n",
    "\n",
    "# Print a padding line\n",
    "#print(\"Padding line\")\n",
    "#for event in sliding_window:\n",
    "#    print(f'Time: {event.get_time()}, Value: {event.get_value()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPAD2 Algorithm ###\n",
    "\n",
    "\"\"\"\n",
    "THE PLAN\n",
    "\n",
    "Making the program start from any predefined time, and then continue to fetch data from that time and onwards.\n",
    "And so it can process older data and then catch up to the present time.\n",
    "\n",
    "It therefore processes a batch of 3 from the earliest timestamp in the range, and then updates the start time for the next iteration \n",
    "by incrementing the start time by a set time to both avoid duplicate events and to eventually catch up to the present time.\n",
    "\n",
    "The program will run indefinitely, and will continue to fetch data from the InfluxDB and process it in batches of 3 until the program is stopped.\n",
    "I there are not enough events for a batch, the program will wait for a set time before trying again.\n",
    "When a batch of three is available it will be processed and the it will again wait for another event to be available.\n",
    "\n",
    "This way it is both flexible and efficient, and can be easily used to process either data in real-time or historical data\n",
    "\n",
    "\n",
    "For each batch it follows the algorithm of RePAD2\n",
    "\n",
    "\n",
    "To-Do:\n",
    "\n",
    "Store actual values (Store whole event?) with predicted values in sliding window \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Testing of InfluxDB with LSTM\n",
    "# Må installeres influxdb-client \"pip install influxdb-client\"\n",
    "# https://www.influxdata.com/blog/getting-started-with-python-and-influxdb-v2-0/\n",
    "influxdb_url = \"http://localhost:8086\"\n",
    "token = \"random_token\"\n",
    "username = \"influx-admin\"\n",
    "password = \"ThisIsNotThePasswordYouAreLookingFor\"\n",
    "org = \"ORG\"\n",
    "bucket = \"system_state\"\n",
    "\n",
    "# Instantiate the QueryAPI\n",
    "client = InfluxDBClient(url=influxdb_url, token=token, org=org, username=username, password=password)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Sliding window for Threshold\n",
    "sliding_window = deque(maxlen=8064)\n",
    "sliding_window_AARE = deque(maxlen=8064)\n",
    "\n",
    "# Time parameters\n",
    "poll_interval = 1\n",
    "time_increment = 1\n",
    "start_time = \"2014-04-10T00:00:00Z\"\n",
    "\n",
    "T = 0\n",
    "is_first_batch = True\n",
    "\n",
    "# RePAD2 specific\n",
    "flag = True \n",
    "M = None # Model\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Construct the Flux query\n",
    "    query = f'''\n",
    "    from(bucket: \"{bucket}\")\n",
    "     |> range(start: time(v: \"{start_time}\"))\n",
    "     |> filter(fn: (r) => r[\"_measurement\"] == \"cpu_utilization\")\n",
    "    '''\n",
    "        \n",
    "    # Query the data\n",
    "    events = list(query_api.query_stream(org=org, query=query))\n",
    "\n",
    "    if len(events) >= 4: # Need at least 4 to predict next and compare\n",
    "        for i in range(len(events) - 3):\n",
    "            batch_events = events[i:i+3]\n",
    "            next_event = events[i+3]\n",
    "\n",
    "            T += 1\n",
    "\n",
    "            print(f'T: {T}')\n",
    "\n",
    "        # Set T to the length of the sliding \n",
    "            if T >= 2 and T < 5:\n",
    "                M = train_model([event.get_value() for event in list(batch_events)])\n",
    "                pred_D_T_plus_1 = M.predict_next()\n",
    "                print(f'Prediction: {pred_D_T_plus_1}')\n",
    "                # Append the event and its prediction to the sliding window\n",
    "                sliding_window.append((next_event, pred_D_T_plus_1))\n",
    "\n",
    "            elif T >= 5 and T < 7:\n",
    "                # Make lists for actual and predicted values\n",
    "                for event, pred in sliding_window:\n",
    "                    actual_values.append(event.get_value())\n",
    "                    predicted_values.append(pred)\n",
    "                # Calculate AARE and append to sliding window\n",
    "                AARE_T = calculate_aare(actual_values, predicted_values)\n",
    "                sliding_window_AARE.append(AARE_T)\n",
    "                M = train_model([event.get_value() for event in list(batch_events)])\n",
    "                pred_D_T_plus_1 = M.predict_next()\n",
    "                # Append the event and its prediction to the sliding window\n",
    "                sliding_window.append((next_event, pred_D_T_plus_1))\n",
    "\n",
    "            elif T >= 7 and flag == True:\n",
    "                if T != 7: # Use M to precdict D_T\n",
    "                    pred_D_T = M.predict_next()\n",
    "                    # Append the event and its prediction to the sliding window\n",
    "                    sliding_window.append((next_event, pred_D_T))\n",
    "\n",
    "                # Make lists for actual and predicted values\n",
    "                for event, pred in sliding_window:\n",
    "                    actual_values.append(event.get_value())\n",
    "                    predicted_values.append(pred)\n",
    "\n",
    "                # Calculate AARE and append to sliding window\n",
    "                AARE_T = calculate_aare(actual_values, predicted_values)\n",
    "                sliding_window_AARE.append(AARE_T)\n",
    "                # Calculate Thd\n",
    "                Thd = calculate_threshold(sliding_window_AARE)\n",
    "                \n",
    "                if AARE_T <= Thd: pass # Calculate AARE and append to sliding window\n",
    "                else:\n",
    "                    # Pop the right of the sliding window\n",
    "                    sliding_window.pop()\n",
    "                    # Train an LSTM model with D_T-3, D_T-2, D_T-1\n",
    "                    model = train_model([event.get_value() for event in list(batch_events)])\n",
    "                    # Use the model to predict D_T\n",
    "                    pred_D_T = model.predict_next()\n",
    "                    # Append the event and its prediction to the sliding window\n",
    "                    sliding_window.append((next_event, pred_D_T))\n",
    "                    for event, pred in sliding_window:\n",
    "                        actual_values.append(event.get_value())\n",
    "                        predicted_values.append(pred)\n",
    "                    # Re-calculate AARE_T\n",
    "                    AARE_T = calculate_aare(actual_values, predicted_values)\n",
    "                    sliding_window_AARE.append(AARE_T)\n",
    "                    # Re-calculate Thd\n",
    "                    Thd = calculate_threshold(AARE_T)\n",
    "\n",
    "                    if AARE_T <= Thd:\n",
    "                        # D_T is not reported as anomaly\n",
    "                        # Replace M with the new model\n",
    "                        M = model\n",
    "                        # Update flag to True\n",
    "                        flag = True\n",
    "\n",
    "                    else:\n",
    "                        # D_T reported as anomaly immediately\n",
    "                        report_anomaly(next_event, write_api)\n",
    "                        # Update flag to False\n",
    "                        flag = False\n",
    "            \n",
    "            elif T >= 7 and flag == False:\n",
    "                # Train an LSTM model with D_T-3, D_T-2, D_T-1\n",
    "                model = train_model([event.get_value() for event in list(batch_events)])\n",
    "                # Use the model to predict D_T\n",
    "                pred_D_T = model.predict_next()\n",
    "                # Append the event and its prediction to the sliding window\n",
    "                sliding_window.append((next_event, pred_D_T))\n",
    "                for event, pred in sliding_window:\n",
    "                    actual_values.append(event.get_value())\n",
    "                    predicted_values.append(pred)\n",
    "                # Calculate AARE_T\n",
    "                AARE_T = calculate_aare(actual_values, predicted_values)\n",
    "                sliding_window_AARE.append(AARE_T)\n",
    "                # Calculate Thd\n",
    "                Thd = calculate_threshold(AARE_T)\n",
    "\n",
    "                if AARE_T <= Thd:\n",
    "                    # D_T is not reported as anomaly\n",
    "                    # Replace M with the new model\n",
    "                    M = model\n",
    "                    # Update flag to True\n",
    "                    flag = True\n",
    "\n",
    "                else:\n",
    "                    # D_T reported as anomaly immediately\n",
    "                    report_anomaly(next_event)\n",
    "                    # Update flag to False\n",
    "                    flag = False\n",
    "\n",
    "\n",
    "            # After processing the event\n",
    "            if i + 3 >= len(events):\n",
    "                # Update start time for the next iteration\n",
    "                last_event_time = batch_events[-1].get_time()\n",
    "                # Increment by 1 second to avoid duplicate events\n",
    "                start_time = (last_event_time + timedelta(seconds=time_increment)).isoformat()\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        print(\"No events found in range.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(poll_interval)\n",
    "\n",
    "# Print a padding line\n",
    "#print(\"Padding line\")\n",
    "#for event in sliding_window:\n",
    "#    print(f'Time: {event.get_time()}, Value: {event.get_value()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "THE PLAN\n",
    "\n",
    "Making the program start from any predefined time, and then continue to fetch data from that time and onwards.\n",
    "And so it can process older data and then catch up to the present time.\n",
    "\n",
    "It therefore processes a batch of 3 from the earliest timestamp in the range, and then updates the start time for the next iteration \n",
    "by incrementing the start time by a set time to both avoid duplicate events and to eventually catch up to the present time.\n",
    "\n",
    "The program will run indefinitely, and will continue to fetch data from the InfluxDB and process it in batches of 3 until the program is stopped.\n",
    "I there are not enough events for a batch, the program will wait for a set time before trying again.\n",
    "When a batch of three is available it will be processed and the it will again wait for another event to be available.\n",
    "\n",
    "This way it is both flexible and efficient, and can be easily used to process either data in real-time or historical data\n",
    "\n",
    "\n",
    "For each batch it follows the algorithm of RePAD2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Testing of InfluxDB with LSTM\n",
    "# Må installeres influxdb-client \"pip install influxdb-client\"\n",
    "# https://www.influxdata.com/blog/getting-started-with-python-and-influxdb-v2-0/\n",
    "influxdb_url = \"http://localhost:8086\"\n",
    "token = \"random_token\"\n",
    "username = \"influx-admin\"\n",
    "password = \"ThisIsNotThePasswordYouAreLookingFor\"\n",
    "org = \"ORG\"\n",
    "bucket = \"system_state\"\n",
    "\n",
    "# Instantiate the QueryAPI\n",
    "client = InfluxDBClient(url=influxdb_url, token=token, org=org, username=username, password=password)\n",
    "query_api = client.query_api()\n",
    "\n",
    "# Sliding window for Threshold\n",
    "sliding_window = deque(maxlen=8064)\n",
    "\n",
    "# Time parameters\n",
    "poll_interval = 1\n",
    "time_increment = 1\n",
    "start_time = \"2014-04-10T00:00:00Z\"\n",
    "\n",
    "batch_nr = 0\n",
    "is_first_batch = True\n",
    "\n",
    "# RePAD2 specific\n",
    "flag = True\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Construct the Flux query\n",
    "    query = f'''\n",
    "    from(bucket: \"{bucket}\")\n",
    "     |> range(start: time(v: \"{start_time}\"))\n",
    "     |> filter(fn: (r) => r[\"_measurement\"] == \"cpu_utilization\")\n",
    "    '''\n",
    "        \n",
    "    # Query the data\n",
    "    events = list(query_api.query_stream(org=org, query=query))\n",
    "\n",
    "    if events:\n",
    "        # Process batches of 3 until there are not enough events for a batch\n",
    "        for i in range(len(events) - 2):\n",
    "            # Select the batch of 3 events\n",
    "            batch_events = events[i:i+3]\n",
    "\n",
    "            if is_first_batch:\n",
    "                for event in batch_events:\n",
    "                    sliding_window.append(event) # Storing all events in batch\n",
    "                is_first_batch = False\n",
    "\n",
    "            else:\n",
    "                # For subsequent batches, append only the last event\n",
    "                sliding_window.append(batch_events[-1])\n",
    "\n",
    "##############################################################################\n",
    "            # All RePAD2 processing goes here (I think)\n",
    "            \n",
    "\n",
    "            T = len(sliding_window)-1 # T is the index of the last event in the sliding window\n",
    "\n",
    "            if T >= 2 & T < 5:\n",
    "                # Train LSTM model using last 3 events in sliding window \n",
    "                # Always set Y = 1, 2, 3 ?\n",
    "                # Let M be the resulting model and use it to predict D_T+1\n",
    "                pass\n",
    "\n",
    "            elif T >= 5 & T < 7:\n",
    "                # Calculate AARE_T\n",
    "                # Train LSTM model using last 3 events in sliding window\n",
    "                # Let M be the resulting model and use it to predict D_T+1\n",
    "                pass\n",
    "\n",
    "            elif T >= 7 & flag == True:\n",
    "                if T != 7: # Use M to precdict D_T\n",
    "                    pass\n",
    "                # Calculate AARE_T\n",
    "                # Calculate Thd\n",
    "                \n",
    "                if AARE_T <= Thd:\n",
    "                    # D_T is not reported as anomaly\n",
    "                    pass\n",
    "            \n",
    "                else:\n",
    "                    # Train an LSTM model with the last 3 events in the sliding window\n",
    "                    # Use the model to predict D_T\n",
    "                    # Re-calculate AARE_T\n",
    "                    # Re-calculate Thd\n",
    "                    if AARE_T <= Thd:\n",
    "                        # D_T is not reported as anomaly\n",
    "                        # Replace M with the new model\n",
    "                        # Update flag to True\n",
    "                        pass\n",
    "\n",
    "                    else:\n",
    "                        # D_T reported as anomaly immediately\n",
    "                        # Update flag to False\n",
    "                        pass\n",
    "            \n",
    "            elif T >= 7 & flag == False:\n",
    "                # Train an LSTM model with D_T-3, D_T-2, D_T-1\n",
    "                # Use the model to predict D_T\n",
    "                # Calculate AARE_T\n",
    "                # Calculate Thd\n",
    "\n",
    "                if AARE_T <= Thd:\n",
    "                    # D_T is not reported as anomaly\n",
    "                    # Replace M with the new model\n",
    "                    # Update flag to True\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    # D_T reported as anomaly immediately\n",
    "                    # Update flag to False\n",
    "                    pass\n",
    "\n",
    "\n",
    "            # Example of processing the events\n",
    "            # Print batch nr as seperator\n",
    "            #batch_nr += 1\n",
    "            #print(\"Batch nr: \", batch_nr)\n",
    "            #for event in batch_events:\n",
    "                # Print the timestamp and value of the event\n",
    "                #print(f'Time: {event.get_time()}, Value: {event.get_value()}')\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "            # After processing the event\n",
    "            if i + 3 >= len(events):\n",
    "                # Update start time for the next iteration\n",
    "                last_event_time = batch_events[-1].get_time()\n",
    "                # Increment by 1 second to avoid duplicate events\n",
    "                start_time = (last_event_time + timedelta(seconds=time_increment)).isoformat()\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        print(\"No events found in range.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(poll_interval)\n",
    "\n",
    "# Print a padding line\n",
    "#print(\"Padding line\")\n",
    "for event in sliding_window:\n",
    "    print(f'Time: {event.get_time()}, Value: {event.get_value()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
